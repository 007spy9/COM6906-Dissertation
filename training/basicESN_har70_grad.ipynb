{
 "cells": [
  {
   "cell_type": "code",
   "id": "d13b3f78fb377c7b",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-11T15:54:14.874100Z",
     "start_time": "2024-08-11T15:54:14.343025Z"
    }
   },
   "source": [
    "import collections\n",
    "# Prepare paths to local utilities\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T15:54:14.890274Z",
     "start_time": "2024-08-11T15:54:14.875602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "is_kaggle = (os.environ.get(\"PWD\", \"\") == \"/kaggle/working\")\n",
    "print(f\"Are we running in Kaggle? {is_kaggle}\")"
   ],
   "id": "3395053909eb809f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we running in Kaggle? False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T15:54:30.751059Z",
     "start_time": "2024-08-11T15:54:14.892279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not is_kaggle:\n",
    "    models_path = os.path.abspath(os.path.join('..', 'model'))\n",
    "    utils_path = os.path.abspath(os.path.join('..', 'util'))\n",
    "    sys.path.append(models_path)\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    from download.DataDownloader import DataDownloader\n",
    "    from collect.DataframeCollector import DataframeCollector\n",
    "    from collect.TestSetSplitter import TestSetSplitter\n",
    "    from collect.DatasetPreparation import DatasetPreparation\n",
    "    from processing.DataPreprocessor import DataPreprocessor\n",
    "    from reservoir.BasicESNCuda import BasicESNCuda as BasicESN\n",
    "    from reservoir.ESNUtil import generate_input_weights\n",
    "\n",
    "else:\n",
    "    from datadownloader.datadownloader import DataDownloader\n",
    "    from dataframecollector.dataframecollector import DataframeCollector\n",
    "    from testsetsplitter.testsetsplitter import TestSetSplitter\n",
    "    from datasetpreparation.datasetpreparation import DatasetPreparation\n",
    "    from datapreprocessor.datapreprocessor import DataPreprocessor\n",
    "    from basicesncuda.basicesncuda import BasicESNCuda as BasicESN\n",
    "    from esnutil.esnutil import generate_input_weights"
   ],
   "id": "33764a292bff8492",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T15:54:32.447945Z",
     "start_time": "2024-08-11T15:54:30.752564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    '''\n",
    "    Context manager to patch joblib to report into tqdm progress bar given as argument\n",
    "    :param tqdm_object: The tqdm progress bar\n",
    "    '''\n",
    "\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ],
   "id": "6f5a44a9b60d0c0b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T15:54:47.323795Z",
     "start_time": "2024-08-11T15:54:32.450864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_preparation = DatasetPreparation()\n",
    "\n",
    "input_features = ['back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z']\n",
    "output_features = ['label']\n",
    "\n",
    "data_preparation.prepare_dataset('har70plus', input_features, output_features)\n",
    "X_train_scaled, y_train_encoded = data_preparation.get_preprocessed_data('train')\n",
    "X_val_scaled, y_val_encoded = data_preparation.get_preprocessed_data('val')\n",
    "X_test_scaled, y_test_encoded = data_preparation.get_preprocessed_data('test')\n",
    "\n",
    "data_preprocessor = DataPreprocessor()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Discovered  18  csv files in  E:\\PyCharm\\COM6906-Dissertation\\data\\har70plus\n",
      "Loading the csv files into dataframes\n",
      "Loaded  18  dataframes\n",
      "Concatenating the dataframes\n",
      "Data shape:  [(103860, 9), (131367, 9), (116413, 9), (150758, 9), (87006, 9), (122714, 9), (120125, 9), (130494, 9), (121763, 9), (122061, 9), (128063, 9), (119310, 9), (123599, 9), (101510, 9), (153517, 9), (138278, 9), (147045, 9), (141714, 9)]\n",
      "Number of frames in training set: 17\n",
      "Number of frames in validation set: 17\n",
      "Number of frames in testing set: 18\n",
      "X_train shape: (1357646, 6), Y_train shape: (1357646,)\n",
      "X_val shape: (339404, 6), Y_val shape: (339404,)\n",
      "X_test shape: (562547, 6), Y_test shape: (562547,)\n",
      "Y_train encoded shape: (1357646, 7)\n",
      "Y_val encoded shape: (339404, 7)\n",
      "Y_test encoded shape: (562547, 7)\n",
      "Y_train decoded shape: (1357646, 1)\n",
      "Y_val decoded shape: (339404, 1)\n",
      "Y_test decoded shape: (562547, 1)\n",
      "X_train scaled shape: (1357646, 6)\n",
      "X_val scaled shape: (339404, 6)\n",
      "X_test scaled shape: (562547, 6)\n",
      "Class: 1, Weight: 0.2990790164739866\n",
      "Class: 3, Weight: 4.886608305255555\n",
      "Class: 4, Weight: 70.78937969924812\n",
      "Class: 5, Weight: 64.84523331228836\n",
      "Class: 6, Weight: 0.7721461803556264\n",
      "Class: 7, Weight: 0.6676972510788485\n",
      "Class: 8, Weight: 1.5887213012401267\n",
      "Dataset preparation complete.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T15:54:54.681010Z",
     "start_time": "2024-08-11T15:54:47.325694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As we have some working models, we are now going to try using a gradient descent to optimise the reservoir hyperparameters\n",
    "optimised_pickle = 'basicESN_har70.pkl'\n",
    "\n",
    "optimised_esn = None\n",
    "\n",
    "if os.path.exists(optimised_pickle):\n",
    "    with open(optimised_pickle, 'rb') as f:\n",
    "        optimised_esn = pickle.load(f)\n",
    "else:\n",
    "    print('No optimised model found.')"
   ],
   "id": "ea874b02b075ee8a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T16:05:38.902263Z",
     "start_time": "2024-08-11T16:05:30.664494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if optimised_esn is not None:\n",
    "    optimised_esn.fit_gradient_descent(X_train_scaled, y_train_encoded, X_val_scaled, y_val_encoded, max_epochs=5, learning_rate=0.01, batch_size=1000)"
   ],
   "id": "483b40a4e918e475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1132.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of previous_states: torch.Size([1000, 500])\n",
      "Type of state: <class 'torch.Tensor'>\n",
      "Type of y_pred: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m optimised_esn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m----> 2\u001B[0m     \u001B[43moptimised_esn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_gradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\PyCharm\\COM6906-Dissertation\\model\\reservoir\\BasicESNCuda.py:272\u001B[0m, in \u001B[0;36mBasicESNCuda.fit_gradient_descent\u001B[1;34m(self, x, y, x_val, y_val, max_epochs, learning_rate, batch_size)\u001B[0m\n\u001B[0;32m    269\u001B[0m # Backpropagate\n\u001B[0;32m    270\u001B[0m loss.backward()\n\u001B[1;32m--> 272\u001B[0m # Update the parameters\n\u001B[0;32m    273\u001B[0m optimiser.step()\n\u001B[0;32m    275\u001B[0m with torch.no_grad():\n\u001B[0;32m    276\u001B[0m     # Our performance metric will be nmrse\n\u001B[0;32m    277\u001B[0m     # Compute the nmrse\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1184\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1186\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1187\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\training\\lib\\site-packages\\torch\\nn\\functional.py:3086\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3084\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3085\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3086\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
